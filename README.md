# Задание

Поднять отказоустойчивый кластер с фактором репликации 2 или выше для использования rbd, cephfs, s3. Подключить клиентов к созданному хранилищу. Отработать сценарии сбоев.

# Подготовка к запуску

Публичный ключ должен находиться по пути: ~/.ssh/id_rsa.pub

В системе должен быть установлен terraform. Тестировалось на версии terraform v1.6.6.

# Настройка и запуск

* склонировать репозиторий;
* настроить подключение к облаку яндекс. Для этого перейти в каталог ```terraform``` создать файл ```yc.auto.tfvars``` с содержимым (подставить свой токен, id облака и каталога):

```
yc_token     = токен
yc_cloud_id  = id облака
yc_folder_id = id каталога
```

* инициализировать терраформ: ```terraform init```;
* запустить создание инфраструктуры: ```terraform apply```;
* после создания инфраструктуры дождаться загрузки ВМ;
* перейти в каталог ```ansible``` и запустить плейбук: ```ansible-playbook setup.yaml```
  * плейбук подготавливает вм к установке ceph
* после окончания работы плейбука взять публичный ip-адрес из вывода terraform и подключиться к вм ```mon0```
  * данная вм будет являться первым монитором
  * с данной вм будет выполняться установка ceph
* в выводе ansible скопировать команду для развертывания кластера ceph и выполнить на ```mon0```
* дождаться окончания работы команды
* затем скопировать команду для изменения публичных подсетей (пробовал указывать в конфиге, как приведено [здесь](https://www.ibm.com/docs/en/storage-ceph/5?topic=configuration-configuring-multiple-public-networks-cluster), но не заработало)
* перезапустить мониторы командой ```ceph orch restart mon```
* дождаться развертывания кластера

# Расчет кластера
## Просчитать pg для pool'ов:
rbd - 5/10 объема дисков
cephfs - 3/10 объема дисков
объяснить логику расчёта, создать пулы.

Формула для расчета:
Total PGs = (total_number_of_OSD * %_usage) / max_replication_count

Получаем для 4 osd с дисками по 10 Гб:

Для RBD: total PGs = (4 * 50) / 3 = 66,7 => 128 pg

Создаем пул: ```ceph osd pool create otus_rbd 128```

Для cephfs: total PGs = (4 * 30) / 3 = 40 => 64 pg

Создаем пул для данных: ```ceph osd pool create otus_cephfs.data 64```
и для метаданных: ```ceph osd pool create otus_cephfs.meta```

# Создать и пробросить на клиентские машины
3 rbd
cephfs (общий раздел на каждую машину)

## RBD
Создаем 3 rbd диска
rbd create disk1 --size 1G --pool otus_rbd
rbd create disk2 --size 2G --pool otus_rbd
rbd create disk3 --size 3G --pool otus_rbd

## Cephfs
Создаем cephfs

```ceph fs new cephfs otus_cephfs.meta otus_cephfs.data```

TODO: подключение

# Аварии и масштабирование.
## Сгенерировать split-brain, посмотреть поведение кластера, решить проблему (результат - запись консоли с выполнением)
## Сгенерировать сбой ноды с osd, вывести из кластера, добавить новую

Останавливаем хост ```osd3```.
Удаляем хост из кластера, т.к. он больше недоступен: ```ceph orch host rm osd3 --offline --force```

## Сгенерировать сбой/обслуживание серверной/дата центра, проверить работоспособность сервисов (результат - запись консоли)
## Расширить кластер на 2+osd, сделать перерасчёт pg, объяснить логику
## Уменьшить кластер на 1+osd, сделать перерасчёт pg, объяснить логику








